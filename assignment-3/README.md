Read the "[Benchmarking data science: Twelve ways to lie with statistics and performance on parallel computers.](https://htor.inf.ethz.ch/publications/img/hoefler-12-ways-data-science-preprint.pdf)" article. It covers 12 common mistakes papers make when reporting DL performance results. 
After reading the article pick one of the following papers we covered this semester and evaluate its results and reporting in light of these 12 common mistakes.

- Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism
- Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM
- H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models
- Deep learning with COTS HPC systems

What you should turn in: A 2-4 page PDF that discusses each of the twelve points (numbered 1-12). For each point write a short paragraph discussing whether the paper falls into that trap or not. This may not be yes/no so please elaborate. If the point does not apply at all, then briefly explain why. For the inference paper(s) you can consider inference accuracy/fidelity in place of training accuracy.

If you're curious there's [a related, older article](https://htor.inf.ethz.ch/blog/index.php/2018/11/08/twelve-ways-to-fool-the-masses-when-reporting-performance-of-deep-learning-workloads/) with slightly different points about DL.
This trend was started by [D. Bailey's 1991 article](https://www.davidhbailey.com/dhbpapers/twelve-ways.pdf) on 12 ways parallel computing performance results are typically misreported; this is a good read if you're interested.
